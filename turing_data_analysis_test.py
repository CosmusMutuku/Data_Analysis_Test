# -*- coding: utf-8 -*-
"""Turing Data Analysis Test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MXSxizxud2Xhl61hQC_BcJPYl9kVA2nl
"""

import pandas as pd

#Load Data
covid = "/content/covid_data.csv"
cardioa='/content/cardio_alco.csv'
cardiob="/content/cardio_base.csv"

#Read data
df = pd.read_csv(covid)
dfa = pd.read_csv(cardioa)
dfb = pd.read_csv(cardiob)

#Display df
df.head(25)

#display dfa
dfa.head()

#display dfb
dfb.head(20)

"""### Checking Data types"""

#Check the data types
df.info()

#Convert date column to be in datetime
df['date'] = pd.to_datetime(df['date'])
#check the new datatypes
df.info()



dfa.info(())

dfb.info()

#Listing the unique values in the gender to know whether its only 0 and 1
# Assuming 'dfb' is your DataFrame
unique_gender_values = dfb['gender'].unique().tolist()

# Check if the list of unique values contains only 2 and 1
if set(unique_gender_values) == {1, 2}:
    print("The 'gender' column contains only 1 and 2.")
else:
    print("The 'gender' column contains other values as well.")

#Convert the gender column into binary
dfb['gender'] = dfb['gender'].map({1: 0, 2: 1})

#Check the new data types
dfb.info()

#Checking non-integer values
dfb['gender'].value_counts()

#Checking the unique values in the cholesterol
unique_cholesterol_values = dfb['cholesterol'].unique().tolist()
unique_cholesterol_values

#Checking the unique values in the smoke
unique_smoke_values = dfb['smoke'].unique().tolist()
unique_smoke_values

"""### Checking the Missing Values"""

#check the missing values
df.isnull().sum()

#What are these countries whose population is missing
rows_with_missing_population = df[df['population'].isnull()]
locations_with_missing_population = rows_with_missing_population['location'].unique()

print("Countries with missing population values:")
for location in locations_with_missing_population:
    print(location)

#Any row with values in Population column and has international under location

rows_with_international_location = df[df['location'] == 'International']
rows_with_international_location_and_population = rows_with_international_location[~rows_with_international_location['population'].isnull()]

if not rows_with_international_location_and_population.empty:
    print("There are rows with 'International' location and non-null population values.")
else:
    print("All rows with 'International' location have missing population values.")

#See countries where gdp pper capital is missing

locations_with_missing_gdp_per_capita = df[df['gdp_per_capita'].isnull()]['location'].unique()

print("Countries/locations with missing GDP per capita values:")
for location in locations_with_missing_gdp_per_capita:
    print(location)

#Any of these countries that we have gdp_per_capita given already in previous rows?
locations_with_missing_gdp_per_capita = df[df['gdp_per_capita'].isnull()]['location'].unique()

# Filter the DataFrame for rows where 'location' is in the list of locations with missing GDP per capita
rows_with_missing_gdp_per_capita = df[df['location'].isin(locations_with_missing_gdp_per_capita)]

# Check if there are any non-null values in the 'gdp_per_capita' column for these rows
rows_with_non_null_gdp = rows_with_missing_gdp_per_capita[~rows_with_missing_gdp_per_capita['gdp_per_capita'].isnull()]

if not rows_with_non_null_gdp.empty:
    print("There are rows with non-null GDP per capita values for the countries with missing values.")
    print(rows_with_non_null_gdp[['location', 'gdp_per_capita']])
else:
    print("There are no rows with non-null GDP per capita values for the countries with missing values.")

# List of countries/locations with missing GDP per capita
countries_with_missing_gdp = [
    'Andorra', 'Anguilla', 'Bonaire Sint Eustatius and Saba',
    'British Virgin Islands', 'Cuba', 'Curacao', 'Faeroe Islands',
    'Falkland Islands', 'French Polynesia', 'Gibraltar', 'Greenland',
    'Guam', 'Guernsey', 'Isle of Man', 'Jersey', 'Liechtenstein',
    'Monaco', 'Montserrat', 'New Caledonia', 'Northern Mariana Islands',
    'Somalia', 'Syria', 'Taiwan', 'Turks and Caicos Islands',
    'United States Virgin Islands', 'Vatican', 'Western Sahara', 'International'
]

# Filter the DataFrame for rows where 'location' is in the list of countries with missing GDP per capita
rows_for_missing_gdp = df[df['location'].isin(countries_with_missing_gdp)]

# Display only the 'location' and 'gdp_per_capita' columns for these rows
desired_columns = ['location', 'gdp_per_capita']
print(rows_for_missing_gdp[desired_columns])

# Filter the displayed rows to show only where 'gdp_per_capita' is non-null
non_null_gdp_rows = rows_for_missing_gdp.dropna(subset=['gdp_per_capita'])

# Display only the 'location' and 'gdp_per_capita' columns for these non-null rows
desired_columns = ['location', 'gdp_per_capita']
print(non_null_gdp_rows[desired_columns])

"""Therefore GDP Per capita is unknown for these Locations: Andorra
Anguilla
Bonaire Sint Eustatius and Saba
British Virgin Islands
Cuba
Curacao
Faeroe Islands
Falkland Islands
French Polynesia
Gibraltar
Greenland
Guam
Guernsey
Isle of Man
Jersey
Liechtenstein
Monaco
Montserrat
New Caledonia
Northern Mariana Islands
Somalia
Syria
Taiwan
Turks and Caicos Islands
United States Virgin Islands
Vatican
Western Sahara

###Now the hospital beds
"""

# Calculate the average 'hospital_beds_per_thousand' for each location
location_avg_beds = df.groupby('location')['hospital_beds_per_thousand'].mean().reset_index()

# Merge the calculated averages back into the original DataFrame where 'hospital_beds_per_thousand' is NaN
df = df.merge(location_avg_beds, on='location', suffixes=('', '_avg'), how='left')

# Fill NaN values in 'hospital_beds_per_thousand' with the calculated averages
df['hospital_beds_per_thousand'].fillna(df['hospital_beds_per_thousand_avg'], inplace=True)

# Drop the extra 'hospital_beds_per_thousand_avg' column
df.drop(columns='hospital_beds_per_thousand_avg', inplace=True)

#Now lets see the missing values
df.isnull().sum()

"""Seems the missing values are just unknown and there wasn't an error because there is none that has been replaced. Now let just see the locations where these values are missing"""

locations_with_missing_beds = df[df['hospital_beds_per_thousand'].isnull()]['location'].unique()

print("Countries/locations with missing hospital beds per thousand values:")
for location in locations_with_missing_beds:
    print(location)

# Let's zoom in and see the two columns in the locations
# List of countries/locations with missing hospital beds per thousand values
countries_with_missing_beds = [
    'Andorra', 'Angola', 'Anguilla', 'Aruba', 'Bermuda',
    'Bonaire Sint Eustatius and Saba', 'British Virgin Islands',
    'Cayman Islands', 'Chad', 'Congo', "Cote d'Ivoire", 'Curacao',
    'Democratic Republic of Congo', 'Faeroe Islands', 'Falkland Islands',
    'French Polynesia', 'Gibraltar', 'Greenland', 'Guam', 'Guernsey',
    'Guinea-Bissau', 'Hong Kong', 'Isle of Man', 'Jersey', 'Kosovo',
    'Lesotho', 'Maldives', 'Mauritania', 'Montserrat', 'Namibia',
    'New Caledonia', 'Nigeria', 'Northern Mariana Islands', 'Palestine',
    'Papua New Guinea', 'Puerto Rico', 'Rwanda', 'Senegal', 'Sierra Leone',
    'Sint Maarten (Dutch part)', 'South Sudan', 'Taiwan',
    'Turks and Caicos Islands', 'United States Virgin Islands', 'Vatican',
    'Western Sahara', 'International'
]

# Filter the DataFrame to show only the 'location' and 'hospital_beds_per_thousand' columns for these countries
desired_columns = ['location', 'hospital_beds_per_thousand']
rows_for_missing_beds = df[df['location'].isin(countries_with_missing_beds)][desired_columns]

# Display the DataFrame
print(rows_for_missing_beds)

non_null_beds_rows = rows_for_missing_beds[~rows_for_missing_beds['hospital_beds_per_thousand'].isnull()]

# Display the DataFrame
print(non_null_beds_rows)

"""Therefore it wasn't an error, the data was just unkown. It is difficult to fill these values for now

###Aged 65
"""

# Calculate the mean 'aged_65_older_percent' for each location
location_avg_65_percent = df.groupby('location')['aged_65_older_percent'].mean().reset_index()

# Merge the calculated means back into the original DataFrame where 'aged_65_older_percent' is NaN
df = df.merge(location_avg_65_percent, on='location', suffixes=('', '_avg'), how='left')

# Fill NaN values in 'aged_65_older_percent' with the calculated means
df['aged_65_older_percent'].fillna(df['aged_65_older_percent_avg'], inplace=True)

# Drop the extra 'aged_65_older_percent_avg' column
df.drop(columns='aged_65_older_percent_avg', inplace=True)

df.isnull().sum()

"""Still the data was just unknown"""

dfa.isnull().sum()

dfb.isnull().sum()

#check the descriptive statistics
df.describe()

dfa.describe()

dfb.describe()

#Check Unique Values
df.nunique()

dfa.nunique()

dfb.nunique()

# Assuming 'dfb' is your DataFrame
import numpy as np

# Calculate the 99th percentile of the 'height' column
height_99th_percentile = np.percentile(dfb['height'], 99)

# Filter the DataFrame to select rows where 'height' is greater than or equal to the 99th percentile
tallest_1_percent = dfb[dfb['height'] >= height_99th_percentile]

# Display the height of the tallest 1% of the people
print("Height of the tallest 1% of people:", height_99th_percentile)

# Assuming 'dfb' is your DataFrame
import pandas as pd

# Calculate the Spearman rank correlation matrix for all features
spearman_corr_matrix = dfb.corr(method='spearman')

# Exclude self-correlations and duplicates by masking the upper triangle of the matrix
mask = np.triu(np.ones_like(spearman_corr_matrix, dtype=bool))
spearman_corr_matrix = spearman_corr_matrix.mask(mask)

# Find the two features with the highest absolute Spearman rank correlation
top_corr_features = spearman_corr_matrix.abs().unstack().sort_values(ascending=False)

# The first two elements in 'top_corr_features' will have the highest absolute correlation values
feature1, feature2 = top_corr_features.index[0]
correlation_value = top_corr_features[0]

# Display the two features and their correlation value
print("Top two features with the highest Spearman rank correlation:")
print("Feature 1:", feature1)
print("Feature 2:", feature2)
print("Correlation Value:", correlation_value)

# Assuming 'dfb' is your DataFrame
import numpy as np

# Calculate the mean and standard deviation of the 'height' column
mean_height = dfb['height'].mean()
std_dev_height = dfb['height'].std()

# Determine the threshold for being more than 2 standard deviations away from the mean
threshold = mean_height + 2 * std_dev_height

# Filter the DataFrame to select rows where 'height' is beyond the threshold
tall_people = dfb[dfb['height'] > threshold]

# Calculate the percentage of people meeting this criterion
percentage_tall_people = (len(tall_people) / len(dfb)) * 100

print("Percentage of people more than 2 standard deviations from the average height:", percentage_tall_people)

# Read 'dfa' DataFrame with the correct delimiter (semicolon)
dfa = pd.read_csv('/content/cardio_alco.csv', delimiter=';')

# Now, you can proceed with the merge operation
dfn = dfb.merge(dfa, on='id', how='inner')

# Step 2: Filter 'dfn' to include only individuals over 50 years old
dfn_over_50 = dfn[dfn['age'] > 50]

# Step 3: Calculate the percentage of individuals who consume alcohol among those over 50 years old
percentage_over_50_with_alcohol = (len(dfn_over_50) / len(dfb[dfb['age'] > 50])) * 100

print("Percentage of population over 50 years old who consume alcohol:", percentage_over_50_with_alcohol)

# Get the minimum and maximum values of the 'age' column in dfb
min_age = dfb['age'].min()
max_age = dfb['age'].max()

print("Minimum age:", min_age)
print("Maximum age:", max_age)

import pandas as pd
import scipy.stats as stats

# Assuming 'dfb' is your DataFrame
# A: Smokers have higher cholesterol levels than non-smokers
smokers_cholesterol = dfb[dfb['smoke'] == 1]['cholesterol']
non_smokers_cholesterol = dfb[dfb['smoke'] == 0]['cholesterol']

# Perform a t-test for the difference in means
t_stat, p_value = stats.ttest_ind(smokers_cholesterol, non_smokers_cholesterol, equal_var=False)

# Check if the p-value is less than 0.05 (95% confidence level)
if p_value < 0.05:
    print("Statement A is true with 95% confidence.")
else:
    print("Statement A is not true with 95% confidence.")

# B: Smokers weigh less than non-smokers
smokers_weight = dfb[dfb['smoke'] == 1]['weight']
non_smokers_weight = dfb[dfb['smoke'] == 0]['weight']

# Perform a t-test for the difference in means
t_stat, p_value = stats.ttest_ind(smokers_weight, non_smokers_weight, equal_var=False)

# Check if the p-value is less than 0.05 (95% confidence level)
if p_value < 0.05:
    print("Statement B is true with 95% confidence.")
else:
    print("Statement B is not true with 95% confidence.")

# C: Smokers have higher blood pressure than non-smokers
smokers_ap_hi = dfb[dfb['smoke'] == 1]['ap_hi']
non_smokers_ap_hi = dfb[dfb['smoke'] == 0]['ap_hi']

# Perform a t-test for the difference in means
t_stat, p_value = stats.ttest_ind(smokers_ap_hi, non_smokers_ap_hi, equal_var=False)

# Check if the p-value is less than 0.05 (95% confidence level)
if p_value < 0.05:
    print("Statement C is true with 95% confidence.")
else:
    print("Statement C is not true with 95% confidence.")

# D: Men have higher blood pressure than women
male_ap_hi = dfb[dfb['gender'] == 1]['ap_hi']
female_ap_hi = dfb[dfb['gender'] == 2]['ap_hi']

# Perform a t-test for the difference in means
t_stat, p_value = stats.ttest_ind(male_ap_hi, female_ap_hi, equal_var=False)

# Check if the p-value is less than 0.05 (95% confidence level)
if p_value < 0.05:
    print("Statement D is true with 95% confidence.")
else:
    print("Statement D is not true with 95% confidence.")

import pandas as pd

# Assuming 'df' is your DataFrame
# Filter data for Italy and Germany
italy_data = df[df['location'] == 'Italy']
germany_data = df[df['location'] == 'Germany']

# Initialize variables to keep track of cumulative cases
italy_cumulative_cases = 0
germany_cumulative_cases = 0

# Iterate through the rows to find the date
date_difference_more_than_10000 = None
for index, row in italy_data.iterrows():
    italy_cumulative_cases += row['new_cases']
    germany_row = germany_data[germany_data['date'] == row['date']]
    if not germany_row.empty:
        germany_cumulative_cases += germany_row.iloc[0]['new_cases']
    if (italy_cumulative_cases - germany_cumulative_cases) > 10000:
        date_difference_more_than_10000 = row['date']
        break

print("Date when the difference in cumulative cases became more than 10,000:", date_difference_more_than_10000)

import pandas as pd
import numpy as np
from scipy.optimize import curve_fit

# Assuming 'df' is your DataFrame
# Filter data for Italy
italy_data = df[df['location'] == 'Italy']

# Filter data for the date range (from 2020-02-28 to 2020-03-20)
italy_data = italy_data[(italy_data['date'] >= '2020-02-28') & (italy_data['date'] <= '2020-03-20')]

# Calculate the cumulative number of confirmed cases
italy_data['cumulative_cases'] = italy_data['new_cases'].cumsum()

# Define the exponential function
def exponential_func(x, A, B):
    return A * np.exp(B * x)

# Fit the exponential function to the data
x_data = np.arange(len(italy_data))
y_data = italy_data['cumulative_cases']
params, covariance = curve_fit(exponential_func, x_data, y_data)

# Extract the fitted parameters
A, B = params

# Calculate the cumulative cases on 2020-03-20 using the exponential function
days_passed = (pd.to_datetime('2020-03-20') - pd.to_datetime('2020-02-28')).days
predicted_cumulative_cases = exponential_func(days_passed, A, B)

# Find the actual cumulative cases on 2020-03-20
actual_cumulative_cases = italy_data[italy_data['date'] == '2020-03-20']['cumulative_cases'].values[0]

# Calculate the difference between the predicted and actual cumulative cases
difference = actual_cumulative_cases - predicted_cumulative_cases

print("Predicted cumulative cases on 2020-03-20:", predicted_cumulative_cases)
print("Actual cumulative cases on 2020-03-20:", actual_cumulative_cases)
print("Difference between the exponential curve and actual cases on 2020-03-20:", difference)

import pandas as pd

# Assuming 'df' is your DataFrame containing the daily COVID-19 data
# Group the data by country and calculate the total deaths and get the population
country_data = df.groupby('location').agg({'new_deaths': 'sum', 'population': 'first'})

# Calculate the death rate for each country
country_data['death_rate'] = (country_data['new_deaths'] / country_data['population']) * 1_000_000  # Deaths per million inhabitants

# Sort the DataFrame by death rate in descending order
sorted_country_data = country_data.sort_values(by='death_rate', ascending=False)

# Get the country with the 3rd highest death rate
third_highest_death_rate_country = sorted_country_data.iloc[2].name

# Print the result
print("Country with the 3rd highest death rate:", third_highest_death_rate_country)

import pandas as pd
from sklearn.metrics import precision_recall_fscore_support

# Assuming 'df' is your DataFrame containing the COVID-19 data

# Filter the data to exclude rows with missing values for age or death rate
filtered_df = df.dropna(subset=['aged_65_older_percent', 'death_rate'])

# Define a function to classify countries based on the criteria
def classify_country(row):
    if row['aged_65_older_percent'] > 20 and row['death_rate'] > 50:
        return 1  # Positive class
    else:
        return 0  # Negative class

# Apply the classification function to create a binary label column
filtered_df['label'] = filtered_df.apply(classify_country, axis=1)

# Calculate precision, recall, and F1 score
precision, recall, f1_score, _ = precision_recall_fscore_support(
    filtered_df['label'],
    filtered_df['label'],  # Since we're evaluating against the same labels
    average='binary'
)

# Print the F1 score
print("F1 Score:", f1_score)

import pandas as pd

# Assuming 'df' is your DataFrame containing the COVID-19 data

# Calculate the number of countries with GDP > 10,000
countries_with_high_gdp = df[df['gdp_per_capita'] > 10000]

# Calculate the number of countries with at least 5 hospital beds per 10,000 inhabitants
countries_with_high_beds = df[df['hospital_beds_per_thousand'] >= 5]

# Calculate the number of countries that meet both conditions
countries_with_both_conditions = df[(df['gdp_per_capita'] > 10000) & (df['hospital_beds_per_thousand'] >= 5)]

# Calculate the probability of having GDP > 10,000 given at least 5 hospital beds per 10,000 inhabitants
probability = len(countries_with_both_conditions) / len(countries_with_high_beds)

# Print the probability
print("Probability of GDP > 10,000 given at least 5 hospital beds per 10,000 inhabitants:", probability)